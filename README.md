## Google Trends to Neon via Modal

This project fetches Google Trends interest data for keywords and geographies, validates it, and upserts into a Neon Postgres database. It can also write a raw Parquet with staging data to a Modal Volume for traceability. Can be run directly with the Modal CLI or through Docker Compose.

---

Thought process and elaboration at the bottom of this readme

---

## Prerequisites
- Docker desktop and Docker Compose (included with desktop) (for containerized runner)
- Neon Postgres database (connection string)
  - Go to: https://neon.com/ and sign-up
  - Follow steps to create database, then press "connect", select "Connection String" from the drop-down menu when you click on psql, then "Show Password" to get the full connection string
- Modal tokens: Can be generated by heading to https://modal.com/ -> Profile Picture -> Settings -> accessing "API Tokens" tab OR by doing the following:
- Optional: Run through modal cli directly instead of docker
  - Install CLI: `pip install modal`
  - Authenticate: sign-up for modal, then run `modal token new` to generate `MODAL_TOKEN_ID` and `MODAL_TOKEN_SECRET` 
  

### Environment variables 
- **Required** 
  - Regular variables for logging in to modal and calling the app directly
    - `MODAL_TOKEN_ID` (obtained as token-id)
    - `MODAL_TOKEN_SECRET` (obtained as token-secret)
  - Postgres DSN/Connection string, obtained from Neon
    - `NEON_DATABASE_URL` 
- **Optional (Defaults included)**
  - `KEYWORDS` (comma-separated; default `football`)
  - `GEOS` (comma-separated; default `US`)
  - `MODE` (`full` = 90d or `incremental` = 7d; default `full`)
  - `WRITERAW` (`none` or `volume`; default `none`)

---

## Run via Docker Compose

1) Create a `.env` in the same directory as `compose.yml`:
```bash
MODAL_TOKEN_ID=your_modal_token_id
MODAL_TOKEN_SECRET=your_modal_token_secret
NEON_DATABASE_URL=postgres://user:pass@host/db
KEYWORDS=football,basket
GEOS=US,GB
MODE=incremental   
WRITERAW=volume     
```

2) Build and run 
```bash
docker compose up --build
```

Expected output includes a JSON summary like:
```json
{"upserted": 28, "keywords": ["football","basket"], "geos": ["US","GB"], "mode": "incremental", "run_id": "...", "raw_file": "path_to_file"}
```
If `WRITERAW=volume`, `raw_file` will point to `/data/trends/...parquet` inside the Modal Volume `trends_semi_raw`.

3) Re-run with different inputs by editing `.env`

---

## Run directly with Modal CLI (no Docker)
1) Install and authenticate:
```bash
pip install modal
modal token new
export MODAL_TOKEN_ID=... 
export MODAL_TOKEN_SECRET=...
export NEON_DATABASE_URL=postgres://user:pass@host/db
modal secret create neon-dsn NEON_DATABASE_URL="$NEON_DATABASE_URL" --force
```

2) Execute the job:
```bash
cd your_project_directory
modal run app.py --keywords "football,basket" --geos "US,GB" --mode "incremental" --writeraw "none"
```


---

## Verify data in Neon
Use `psql` or a client of your choice. Reminder that ssl is required.
```bash
# Latest rows
psql "$NEON_DATABASE_URL" -c "select * from trends_interest order by date desc limit 10;"

# Counts by geo/keyword
psql "$NEON_DATABASE_URL" -c "select geo, keyword, count(*) from trends_interest group by 1,2 order by 1,2;"
```

---

## Configuration details
- **Modes**
  - `full`: upserts for the last 90 days (based on the freshest date fetched)
  - `incremental`: upserts for the last 7 days
- **Raw export**
  - Set `WRITERAW=volume` (or `--writeraw volume`) to write Parquet to Modal Volume `trends_semi_raw`
  - Files are stored like: `/data/trends/YYYYMMDD/trends_{geoTag}_{keywordTag}_{days}d_{runId}.parquet`
- **Database schema**
  - Final table: `trends_interest(keyword text, geo text, date date, value int, is_partial boolean)`
  - Staging table: `staging_trends_interest(..., run_id uuid, loaded_at timestamptz, mode text, timeframe text, geo_list jsonb, keyword_list jsonb)`
  - Upsert strategy: merge on `(keyword, geo, date)` 

---

## Project structure
- `app.py`: Modal app for data processing, and local entrypoint
- `compose.yml`: Docker Compose service definition with environment variables
- `Dockerfile`: Runner to install Modal CLI and invokes the entrypoint
- `entrypoint.sh`: Creates/updates Modal secret and runs `modal run` with CLI flags
- `.env`: Define in same directory as remaining files if not provided (for testing purposes). Variables defined inside will override defaults 

---

## Approach and design decisions

This will be less structured than the above, just for the sake of conveying my thought process.

- Initial steps: Familiarized with `pytrends`, functions, options, returned objects and the such and how to interpret that data
- Daniel proposed to deploy with Modal, so I started looking into that next and decided to go with it because 1) It looked interesting, 2) Is used in house, 3) Ease of deployment. More on this a little bit later.

Initial plan was to have two services spun up with docker compose, one database service and one service for running our pipeline. Once Modal was suggested I decided to go with that and that had some implications: Switch from local postgres service in docker to hosted DB for ease of access, so I picked Neon as it could be easily integrated into Modal and offered just what I needed. Another option could've been to still spin up a local docker with postgres, and then exposing that to the network through ngrock so my Modal app could interact with it, but I went with the former. 

So, current architecture: Neon for postgres and data persistence, Modal for pipeline needs. Extra: To facilitate deployment I did include a docker compose that does a good bit of the setup, including creating Modal secrets, and the app itself automatically creates a Modal Volume (2nd layer of persistence). API keys and the Neon connection string can be provided for the sake of testing vs having to sign up for those services. Once those two are included the app can be launched with docker compose up --build (tested on macos, nixos, and windows.)
Minimal docker images are used as that aligns with our needs.

Choice of tools: 

- Modal: Already mostly discussed. It is also one way of offering scalability and modularity (getting to build use-case specific images very easily, their underlying infra, secret + volume management ) Very interested in seeing how it's implemented in more production oriented projects. 
- Pandas: is self-explanatory as pytrends returns pandas dataframes, built-in functions help for some usecases (ie: using melt to unpivot dataframes instead of handling that in sql syntax)
- DuckDB: Personal preference, but also makes it very easy to interface with the data frames / translating some of the logic into sql, should the data volume grow or needing to run complex/multi join queries before sending the data to postgres it would handle it better than just pandas operations. It's also perfect for writing our staging data as parquet files for the sake of future-proofing (possible migrations) and archival/traceability (we have data both in Neon AND on modal with a datawarehouse friendly file format). Use cases grow with the data (so long as they realistically fit in memory / disk data is fine too, but wouldn't mean much for this application)
- Pydantic for validating row structure and contents.
- psycopg for connecting to postgres and pushing data there.
- docker and docker compose for deployment.
- misc libraries for helper functions (slug function for naming files to be inserted).


## Pipeline

Logic: (keyword,geo,date) tuples serve as keys. The app creates the tables in our remote database if they don't exist, and defines an index on run_id for the staging table (we will probably want to separate data by runs for some purposes). Staging table is there so we're able to have as much metadata as possible to separate that logic from our final results table, and gives us a fallback table if ever need be. 
Data fetching behavior: We set the mode manually (ie in .env) `full` will fetch the last 90 days with the `today 3-m` timeframe.
Since we get daily data as a result, the behavior for `incremental` 7 day fetches is affected (`now 7-d` returns hourly data, introducing a need to aggregate). What is done instead is that we fetch the last 90 days regardless, and the mode determines how we upsert into our database (upserting only for the last 7 days vs 90.)
Since pytrends gives us relative data to our whole payload per entry, I decided to go with this because building `now 7-d` payloads would kind of scatter the data a lot more than 3-m payloads, you get much more volatile interest values and peaks. Sticking to `today 3-m` keeps our data fresh within the context of said window, giving us better representation of trend evolution on a longer time frame, and we're still minimizing upsert volume (only 7 days). There might be usecases where another approach is preferred based on the business rule.

There is handling for 6+ keyword lists, in the form of stopping the run as it would break data quality/error out (payloads are 5 at a time at most, so introducing a sixth or more keywords would error out or if computed in a separate payload would skew data without proper handling). I have looked into how to handle such keyword needs (anchoring) but it would be overkill.

Data is fetched from pytrends -> manipulated to a more convenient shape (melting the dataframe so we have a unique schema structure instead of multiple keyword columns) -> filtered and types enforced with duckdb -> rows validated with pydantic -> written to postgres into staging table then upserted into final table, as well as written to parquet file in Modal Volume. 

## Extensibility and scalability
- Scheduling: No explicit scheduling, but Modal gives us the option to do cron scheduling. At larger scale, can consider airflow or dagster (as we integrate into more platforms and have several moving parts, becomes more convenient to use a dedicated tool for orchestration)
- Costs and Compute: On the database side, Neon is able to scale as needed and tends to be cheaper than options like RDS, PlanetScale is another interesting offering in this category. Self-hosting on rented compute could also be considered at the cost of having to actively manage the server. Modal is already adopted and seems to work well for all intents and purposes.
- Data variety: Usage of the api here is pretty simple, there's definitely more that can be done (Category specific tables, comparisons of "trend groups", more granular timeframes). Other data sources can be considered (social media app APIs), the staging -> final table approach would go along well (get the data into postgres, even if it varies across sources, then harmonize + upsert into final table)
- Presence of parquet files: Allows us to easily move our data around if needed with proper tooling. Starlake (https://starlake.ai/) integrates very well as a data extraction and movement tool from point A to point B, wether it's from files to DBs, or DBs to file. Scenario: Want to eventually move away from Neon and load into Snowflake without losing historical data and potentially doing some pre-processing, Apache Spark based tools like starlake with dedicated connectors fit perfectly for the job
- Serving the application: Can use built-in Modal capabilities to host an API endpoint to call the application with http requests, could facilitate integration.
